{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Pharma-Safe Lens - Kaggle Validation Notebook\n",
    "\n",
    "**Complete validation for all phases**\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Phase 1-2**: CPU only (no GPU needed)\n",
    "2. **Phase 3+**: Enable GPU accelerator (T4 x2 or P100)\n",
    "\n",
    "## Important: Run cells in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Install System Dependencies (REQUIRED)\n",
    "\n",
    "Tesseract OCR must be installed before Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tesseract OCR engine\n",
    "!apt-get update -y\n",
    "!apt-get install -y tesseract-ocr\n",
    "\n",
    "# Verify\n",
    "!tesseract --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone from GitHub (Replace YOUR_USERNAME)\n",
    "!git clone https://github.com/AdtiyaLingam/pharma-safe-lens.git\n",
    "%cd pharma-safe-lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd backend\n",
    "!pip install -r requirements.txt\n",
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Verify Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/pharma-safe-lens')\n",
    "\n",
    "# Test imports\n",
    "import easyocr\n",
    "import pytesseract\n",
    "import cv2\n",
    "from backend.app.drug_db import DrugDatabase\n",
    "from backend.app.ocr import extract_text\n",
    "from backend.app.interaction_logic import InteractionChecker\n",
    "from backend.app.prompts import PromptTemplates\n",
    "\n",
    "# New in Phase 4\n",
    "from backend.app.safety import SafetyGuard\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 & 2 Validation: Logic Core (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Modules\n",
    "db = DrugDatabase()\n",
    "checker = InteractionChecker()\n",
    "\n",
    "print(f\"Loaded {len(db.drug_map)} drugs\")\n",
    "print(f\"Loaded {len(checker.interactions)} interactions\")\n",
    "\n",
    "# 2. Test Drug Normalization\n",
    "raw_input = ['ECOSPRIN 75', 'WARFARIN 5MG']\n",
    "normalized_drugs = db.normalize(raw_input)\n",
    "print(f\"\\nInput: {raw_input} -> Normalized: {normalized_drugs}\")\n",
    "\n",
    "# 3. Test Interaction Logic\n",
    "interactions = checker.check_multiple(normalized_drugs)\n",
    "for i in interactions:\n",
    "    print(f\"\\n‚ö†Ô∏è RISK FOUND: {i['risk_level'].upper()}\")\n",
    "    print(f\"Reason: {i['clinical_effect']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 Validation: MedGemma Reasoning (GPU REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"‚ùå GPU not detected! Enable Accelerator in Kaggle settings.\")\n",
    "    \n",
    "print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model (Recommend google/gemma-2b-it or 4b-it)\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model {MODEL_ID} Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Inference Pipeline with Chat Templates\n",
    "\n",
    "def generate_with_chat_template(user_prompt):\n",
    "    # Create chat message structure\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=True, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# 1. Generate Explanation\n",
    "explanation = \"\"\n",
    "if interactions:\n",
    "    print(\"üß† Generating Explanation for: Aspirin + Warfarin...\")\n",
    "    \n",
    "    prompt_content = PromptTemplates.format_explanation_prompt(interactions[0])\n",
    "    explanation = generate_with_chat_template(prompt_content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"MEDGEMMA OUTPUT (Raw):\")\n",
    "    print(\"=\"*40)\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 Validation: Safety & Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run Safety Guard\n",
    "print(\"üõ°Ô∏è Running Safety Check...\")\n",
    "is_safe, safe_explanation = SafetyGuard.validate_output(explanation)\n",
    "\n",
    "if is_safe:\n",
    "    print(\"‚úÖ Safety Check Passed.\")\n",
    "else:\n",
    "    print(\"‚ùå Safety Violation Detected!\")\n",
    "    print(f\"Warning: {safe_explanation}\")\n",
    "\n",
    "# 2. Translate to Hindi (Localization)\n",
    "if is_safe:\n",
    "    print(\"\\nüåê Generating Hindi Translation...\")\n",
    "    \n",
    "    trans_prompt = PromptTemplates.format_translation_prompt(safe_explanation, \"Hindi\")\n",
    "    hindi_explanation = generate_with_chat_template(trans_prompt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"HINDI TRANSLATION:\")\n",
    "    print(\"=\"*40)\n",
    "    print(hindi_explanation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
